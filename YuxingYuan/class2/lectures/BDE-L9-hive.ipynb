{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big Data Essentials\n",
    "#### L9:  Hive\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "Yanfei Kang <br>\n",
    "yanfeikang@buaa.edu.cn <br>\n",
    "School of Economics and Management <br>\n",
    "Beihang University <br>\n",
    "http://yanfei.site <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Hive\n",
    "\n",
    "- Hive is Data warehousing tool developed on top of Hadoop Distributed File System (HDFS)\n",
    "- Hive makes job easy for performing operations like\n",
    "    - Data encapsulation\n",
    "    - Ad-hoc queries\n",
    "    - Analysis of huge datasets\n",
    "- Hive provides a mechanism to project structure onto the data and perform queries written in HQL (Hive Query Language)\n",
    "- **Queries or HQL gets converted to map reduce jobs by the Hive compiler**\n",
    "- Targeted towards users who are comfortable with SQL \n",
    "- Hive supports Data Definition Language (*DDL*), Data Manipulation Language (*DML*) and User Defined Functions （*UDF*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Hive?\n",
    "\n",
    "- Many of those low-level details are actually quite repetitive from one job to the next, from low-level chores like wiring together Mappers and Reducers to certain data manipulation constructs, like filtering for just the data you want and performing SQL-like joins on data sets.\n",
    "\n",
    "- Hive not only provides a familiar programming model for people who know SQL, it also eliminates lots of boilerplate and sometimes-tricky coding you would have to do in Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does Hive work?\n",
    "\n",
    "\n",
    "- When MapReduce jobs are required, Hive doesn't generate Java MapReduce programs.    \n",
    "\n",
    "- Instead, it uses built-in, generic Mapper and Reducer modules that are driven by an XML file representing the **job plan**\n",
    "\n",
    "- In other words, these generic modules function like mini language interpreters and the **language** to drive the computation is encoded in XML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to run HQL?\n",
    "\n",
    "- Hive batch\n",
    "- Interactive Hive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hive Batch\n",
    "\n",
    "\n",
    "- Run hive commands from the termial: `hive -e \"SHOW DATABASES;\"`\n",
    "    \n",
    "- Run Hive scripts from the termimal: `hive -f /path/to/file/withqueries.hql`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hive Interactive\n",
    "\n",
    "- Start Hive from a Terminal: `hive`\n",
    "\n",
    "- Execute command within Hive `dfs -ls /;`\n",
    "    \n",
    "- Exit Hive: `exit;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DDL: databases\n",
    "\n",
    "- `SHOW DATABASES;`.\n",
    "- `CREATE DATABASE IF NOT EXISTS myname;` \n",
    "- `DROP DATABASE IF EXISTS myname;`\n",
    "- `CREATE DATABASE IF NOT EXISTS myname LOCATION '/user/yanfei/hive';`\n",
    "- `SHOW DATABASES;`.\n",
    "- `SHOW DATABASES LIKE \"d*\";`.\n",
    "- `USE myname`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DDL: CREATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`CREATE TABLE IF NOT EXISTS yanfeikang.employees (\n",
    "    name\n",
    "        STRING COMMENT 'Employee name',\n",
    "    salary\n",
    "        FLOAT COMMENT 'Employee salary')\n",
    "COMMENT 'Description of the table'\n",
    "TBLPROPERTIES ('creator'='me', 'created_at'='2012-01-02 10:00:00');`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`SHOW TABLES;`\n",
    "\n",
    "`DESCRIBE myname.employees;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DDL: ALTER\n",
    "\n",
    "`ALTER TABLE test_table_1 RENAME TO new_table;`\n",
    "\n",
    "`ALTER TABLE test_table_1 ADD COLUMNS (stock_name string);`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## DDL: DROP\n",
    "\n",
    "`DROP TABLE test_table_1`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DDL: SHOW\n",
    "\n",
    "`SHOW FUNCTIONS [like \"str*\"];`\n",
    "\n",
    "`SHOW DATABASES [like \"h*\"];`\n",
    "\n",
    "`SHOW TABLES [like \"m*\"];`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## DDL: DESCRIBE\n",
    "\n",
    "`DESCRIBE DATABASE bdg_financial;`\n",
    "\n",
    "`DESCRIBE test_table_1;`\n",
    "\n",
    "`DESCRIBE FUNCTION like;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DML: create an external table\n",
    "\n",
    "- Assume we have a data file `stocks.txt` located in HDFS at `/user/yanfei`, we could connect it with Hive as an external table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use yanfeikang;\n",
      "\n",
      "create external table if not exists stocks (\n",
      "    symbol string, \n",
      "    ymd string, \n",
      "    price_open float, \n",
      "    price_high float, \n",
      "    price_low float, \n",
      "    price_close float, \n",
      "    volume int, \n",
      "    price_adj_close float )\n",
      "row format delimited fields terminated by ','\n",
      "LOCATION '/user/yanfei/hive';\n",
      "\n",
      "LOAD DATA INPATH '/data/stocks.txt' OVERWRITE INTO TABLE stocks; \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat code/L9/stocks/stocks.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "'LOCAL' signifies that the input file is on the local file system. If 'LOCAL' is omitted then it looks for the file in HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`LOAD DATA LOCAL INPATH '/home/yanfei/lectures/data/stocks.txt' OVERWRITE INTO TABLE stocks;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where do Hive Tables stores in HDFS?\n",
    "\n",
    "- Hive stores tables files by default at `/user/hive/warehouse` location on HDFS. \n",
    "\n",
    "- On this location, you can find the directories for all databases you create and subdirectories with the table name you use.\n",
    "\n",
    "- While creating Hive tables, you can also specify the custom location where to store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# External/internal tables\n",
    "\n",
    "- For External Tables, Hive stores the data in the hdfs LOCATION specified during creation of the table (generally not in warehouse directory). If the external table is dropped, then the table metadata is deleted but not the data.\n",
    "\n",
    "- For Internal tables, Hive stores data into its warehouse directory. If the table is dropped then both the table metadata and the data will be deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DML: queries\n",
    "\n",
    "- `SELECT avg(price_close) FROM stocks WHERE symbol = 'AAPL';`\n",
    "- `SELECT * FROM stocks WH ymd = '2003-01-02';`\n",
    "- `SELECT symbol, avg(price_close) FROM stocks GROUP BY symbol;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wordcount example using hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use yanfeikang;\n",
      "\n",
      "CREATE EXTERNAL TABLE fruits (line STRING);\n",
      "LOAD DATA INPATH '/user/yanfei/fruits' OVERWRITE INTO TABLE fruits;\n",
      "CREATE TABLE IF NOT EXISTS fruit_counts AS\n",
      "    SELECT name, count(*) AS count from\n",
      "        (SELECT explode(split(line, ' ')) AS name FROM fruits) w\n",
      "    GROUP BY name\n",
      "    ORDER BY name;\n"
     ]
    }
   ],
   "source": [
    "cat code/L9/wordcount/wordcount.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transform: hive and python\n",
    "\n",
    "- When using hive, we can use python to process data. Now let's look at an example.\n",
    "\n",
    "- We will use the  MovieLens 100k data available at `/user/yanfei/u.data`. Read more at [here](http://grouplens.org/datasets/movielens/) about the data.\n",
    "\n",
    "- We can do some analysis with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#! /usr/bin/env python3\n",
      "\n",
      "import sys\n",
      "import datetime\n",
      "\n",
      "for line in sys.stdin:\n",
      "  line = line.strip()\n",
      "  userid, movieid, rating, unixtime = line.split('\\t')\n",
      "  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()\n",
      "  print '\\t'.join([userid, movieid, rating, str(weekday)])\n"
     ]
    }
   ],
   "source": [
    "cat code/L9/movie/weekday_mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use the mapper script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use yanfeikang;\n",
      "\n",
      "\n",
      "CREATE TABLE u_data_new (\n",
      "  userid INT,\n",
      "  movieid INT,\n",
      "  rating INT,\n",
      "  weekday INT)\n",
      "ROW FORMAT DELIMITED\n",
      "FIELDS TERMINATED BY '\\t';\n",
      "\n",
      "CREATE TABLE u_data (\n",
      "  userid INT,\n",
      "  movieid INT,\n",
      "  rating INT,\n",
      "  time INT)\n",
      "ROW FORMAT DELIMITED\n",
      "FIELDS TERMINATED BY '\\t';\n",
      "LOAD DATA INPATH '/user/yanfei/u.data'\n",
      "OVERWRITE INTO TABLE u_data;\n",
      "\n",
      "add FILE hdfs:///user/yanfei/weekday_mapper.py;\n",
      "\n",
      "INSERT OVERWRITE TABLE u_data_new\n",
      "SELECT\n",
      "  TRANSFORM (userid, movieid, rating, unixtime)\n",
      "  USING 'python3 weekday_mapper.py'\n",
      "  AS (userid, movieid, rating, weekday)\n",
      "FROM u_data;\n",
      "\n",
      "SELECT weekday, COUNT(*)\n",
      "FROM u_data_new\n",
      "GROUP BY weekday;\n"
     ]
    }
   ],
   "source": [
    "cat code/L9/movie/wdtrans.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HIVE reference\n",
    "\n",
    "- https://cwiki.apache.org/confluence/display/Hive/Tutorial.\n",
    "- https://sparkbyexamples.com/apache-hive-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lab \n",
    "\n",
    "- Create an external table with Hive for the data `airdelay_small.csv`.\n",
    "\n",
    "- Use the [Hive internal function](https://www.tutorialspoint.com/hive/hive_built_in_functions.htm) do basic statistics as we had with Hadoop.\n",
    "\n",
    "- External Reading: Capriolo, Edward, Dean Wampler, and Jason Rutherglen. Programming Hive: Data warehouse and query language for Hadoop. ” O’Reilly Media, Inc.”, 2012."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "rise": {
   "autolaunch": true,
   "footer": "<h3>Big Data Essential, 2020 Autumn</h3>",
   "scroll": true,
   "theme": "solarized",
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
