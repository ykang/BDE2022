{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big Data Essentials\n",
    "#### L8:  Hadoop Streaming \n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "Yanfei Kang <br>\n",
    "yanfeikang@buaa.edu.cn <br>\n",
    "School of Economics and Management <br>\n",
    "Beihang University <br>\n",
    "http://yanfei.site <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hadoop streaming with Python\n",
    "\n",
    "- Hadoop is written in JAVA, but allows you to write map/reduce code in any language you want using the **Hadoop Streaming** utility. \n",
    "\n",
    "- We have looked at Streaming with Unix commands. In this lecture, you will see how to stream with scripts.\n",
    "\n",
    "- Hadoop Streaming uses Unix standard streams as the interface between Hadoop and your program, so you can use any combination of languages that can read standard input and write to standard output to write your MapReduce program.\n",
    "\n",
    "    - You could use different language in mapper and reduce functions.\n",
    "\n",
    "    - It suits for text processing (e.g. read every line from a big CSV file).\n",
    "\n",
    "    - It can also handle binary streams (e.g. read image as input)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Overall data flow in Hadoop streaming\n",
    "\n",
    "- Like a pipe where data streams through the mapper, the output of which is sorted and streamed through the reducer. In pseudo-code using Unix’s command line notation:\n",
    "\n",
    "`cat ***.txt | mapper | sort | reducer > output`\n",
    " \n",
    "- Best Practice with Hadoop Streaming: write your Hadoop command in a Bash file instead run it directly on Linux Shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop fs -rm -r /user/yanfei/output\n",
      "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
      "    -input /user/yanfei/fruits \\\n",
      "    -output /user/yanfei/output \\\n",
      "    -mapper \"/usr/bin/cat\" \\\n",
      "    -reducer \"/usr/bin/wc\" \n"
     ]
    }
   ],
   "source": [
    "cat code/L8/wordcount-sh/main.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Streaming with Unix commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Streaming with scripts\n",
    "\n",
    "- We can use any executable script that processes a line-oriented data stream from `STDIN` and outputs to `STDOUT` with Hadoop Streaming. \n",
    "\n",
    "- Now let's use patent data from the National Bureau of Economic Research (NBER) at http://www.nber.org/patents/. The data sets were originally compiled for the paper “The NBER Patent Citation Data File: Lessons, Insights and Methodological Tools.” We use the patent description data set apat63_99.txt, and  find maximum of an attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| Variable | Description |\n",
    "| :--- | :--- | \n",
    "| PATENT |Patent number | \n",
    "| GPYEAR |Grant year | \n",
    "| GDATE |Grant date, given as the number of days elapsed since January 1, 1960 | \n",
    "| APPYEAR |Application year (available only for patents granted since 1967) | \n",
    "| COUNTRY |Country of first inventor | \n",
    "| POSTATE |State of first inventory (if country is U.S.)| \n",
    "| CLAIMS |Number of claims (available only for patents granted since 1975) | \n",
    "| NCLASS |3-digit main patent class| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "\n",
      "import sys\n",
      "\n",
      "index = int(sys.argv[1])\n",
      "max   = 0\n",
      "for line in sys.stdin:\n",
      "    fields = line.strip().split(\",\")\n",
      "    if fields[index].isdigit():\n",
      "        val = int(fields[index])\n",
      "        if (val > max):\n",
      "            max = val\n",
      "print(max)\n"
     ]
    }
   ],
   "source": [
    "cat code/L8/patent-max/mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- You can specify any executable as the mapper and/or the reducer. The executables do not need to pre-exist on the machines in the cluster; however, if they don’t, you will need to use **“-file”** option to tell the framework to pack your executable files as a part of job submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#! /usr/bin/sh\n",
      "\n",
      "hadoop fs -rm -r /user/yanfei/output/\n",
      "\n",
      "\n",
      "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
      "    -input /data/apat63_99.txt \\\n",
      "    -output /user/yanfei/output \\\n",
      "    -file mapper.py \\\n",
      "    -mapper \"python3 mapper.py 8\" \\\n"
     ]
    }
   ],
   "source": [
    "cat code/L8/patent-max/main.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The option “-file mapper.py” causes the python executable shipped to the cluster machines as a part of job submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Write a reducer that outputs the maximum over the values output by the mappers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Streaming with key/value pairs\n",
    "\n",
    "- Working with key/value pairs allows us to take advantage of the key-based shuffling and sorting to create interesting data analyses. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Streaming with key/value pairs\n",
    "\n",
    "Now let's consider a wordcount example. \n",
    "\n",
    "- The map step will take the raw text of our document (I will use our lecture notes) and convert it to key/value pairs. Each key is a word, and all keys (words) will have a value of 1.\n",
    "- The reduce step will combine all duplicate keys by adding up their values. Since every key (word) has a value of 1, this will reduce our output to a list of unique keys, each with a value corresponding to that key’s (word’s) count.\n",
    "\n",
    "With Hadoop Streaming, we need to write a program that acts as the mapper and a program that acts as the reducer. These applications must interface with input/output streams in such a way equivalent to the following series of pipes:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`$ head -n100 code/L8/wordcount-py/streaming.ipynb | ./mapper.py | sort | ./reducer.py > output.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The mapper\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\n",
      "\n",
      "import sys\n",
      "\n",
      "for line in sys.stdin:\n",
      "    line = line.strip()\n",
      "    keys = line.split()\n",
      "    for key in keys:\n",
      "        value = 1\n",
      "        print( \"%s\\t%d\" % (key, value) )\n"
     ]
    }
   ],
   "source": [
    "cat code/L8/wordcount-py/mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The shuffle\n",
    "\n",
    "A lot happens between the map and reduce steps that is largely transparent to the developer. In brief, the output of the mappers is transformed and distributed to the reducers (termed the shuffle step) in such a way that\n",
    "\n",
    "- All key/value pairs are sorted before being presented to the reducer function\n",
    "- All key/value pairs sharing the same key are sent to the same reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The reducer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\n",
      "\n",
      "import sys\n",
      "\n",
      "last_key = None\n",
      "running_total = 0\n",
      "\n",
      "for input_line in sys.stdin:\n",
      "    input_line = input_line.strip()\n",
      "    this_key, value = input_line.split(\"\\t\", 1)\n",
      "    value = int(value)\n",
      "    \n",
      "    if last_key == this_key:\n",
      "        running_total += value\n",
      "    else:\n",
      "        if last_key:\n",
      "            print( \"%s\\t%d\" % (last_key, running_total) )\n",
      "        running_total = value\n",
      "        last_key = this_key\n",
      "\n",
      "if last_key == this_key:\n",
      "    print( \"%s\\t%d\" % (last_key, running_total) )\n"
     ]
    }
   ],
   "source": [
    "cat code/L8/wordcount-py/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Running the hadoop job\n",
    "    \n",
    "**NOTE** Before submitting the Hadoop job, you should make sure your mapper and reducer scripts actually work. This is just a matter of running them through pipes on a little bit of sample data: \n",
    "`head -n100 streaming.ipynb | ./mapper.py | sort | ./reducer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop fs -rm -r /user/yanfei/output\n",
      "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
      "    -input /user/yanfei/hadoop.ipynb \\\n",
      "    -output /user/yanfei/output \\\n",
      "    -file mapper.py reducer.py \\\n",
      "    -mapper \"python3 mapper.py\" \\\n",
      "    -reducer \"python3 reducer.py\"\n"
     ]
    }
   ],
   "source": [
    "cat code/L8/wordcount-py/main.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Streaming with key/value pairs\n",
    "\n",
    "Now let us find the average number of claims *for each country*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\n",
      "\n",
      "import sys\n",
      "\n",
      "for line in sys.stdin:\n",
      "    fields = line.split(\",\")\n",
      "    if (fields[9] and fields[9].isdigit()):\n",
      "        print(fields[5][1:-1] + \"\\t\" + fields[9])\n"
     ]
    }
   ],
   "source": [
    "cat code/L8/patent-avg/mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "1. Run the hadoop job.\n",
    "2. Look at your output without the reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\n",
      "\n",
      "import sys\n",
      "\n",
      "(last_key, sum, count) = (None, 0.0, 0)\n",
      "for line in sys.stdin:\n",
      "    (key, val) = line.split(\"\\t\")\n",
      "    if last_key and last_key != key:\n",
      "        print(str(last_key) + \"\\t\" + str(sum / count))\n",
      "        (sum, count) = (0.0, 0)\n",
      "    last_key = key\n",
      "    sum   += float(val)\n",
      "    count += 1\n",
      "print(str(last_key) + \"\\t\" + str(sum / count))\n"
     ]
    }
   ],
   "source": [
    "cat code/L8/patent-avg/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lab \n",
    "\n",
    "- Work with Hadoop streaming for *ANYTHING*, or \n",
    "- Use the airline data located at `/data` on hdfs (see [description](http://stat-computing.org/dataexpo/2009/the-data.html)).\n",
    "    - Extract useful information from the data.\n",
    "        - List all airport codes, with frequency.\n",
    "        - Make a new binary variable (Y) to indicate if a trip is delayed or not.\n",
    "    - Make dummy transformation for variables such as DayofWeek, Month...\n",
    "    - Finally, save your output in a file.\n",
    "        - Each row contains the binary variable (Y), CarrierDelay, and your constructed dummy variables as predictors.\n",
    "        - If possible, save the output in a libsvm [sparse format](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.dump_svmlight_file.html#sklearn.datasets.dump_svmlight_file) to save space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further readings\n",
    "\n",
    "- [Hadoop streaming documentation.](https://hadoop.apache.org/docs/r3.2.1/hadoop-streaming/HadoopStreaming.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "rise": {
   "autolaunch": true,
   "footer": "<h3>Big Data Essential, 2020 Autumn</h3>",
   "scroll": true,
   "theme": "solarized",
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
