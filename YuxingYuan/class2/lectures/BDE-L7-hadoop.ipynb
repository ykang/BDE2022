{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big Data Essentials\n",
    "#### L7: Exploring the World of Hadoop\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "Yanfei Kang <br>\n",
    "yanfeikang@buaa.edu.cn <br>\n",
    "School of Economics and Management <br>\n",
    "Beihang University <br>\n",
    "http://yanfei.site <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Objectives of this lecture\n",
    "1. Introduction to distributed computing\n",
    "2. Discovering Hadoop and why it’s so important\n",
    "2. Exploring the Hadoop Distributed File System\n",
    "3. Digging into Hadoop MapReduce\n",
    "4. Putting Hadoop to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Distributed systems?\n",
    "![](./figs/google.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Hadoop?\n",
    "- Hadoop is a platform that provides both distributed storage and computational capabilities.\n",
    "\n",
    "- Hadoop is a distributed master-worker architecture that consists of the Hadoop Distributed File System (HDFS) for storage and MapReduce for computational capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Explaining Hadoop\n",
    "\n",
    "[Hadoop](http://hadoop.apache.org) was originally built by a Yahoo! engineer named Doug Cutting and is now an open source project managed by the Apache Software Foundation.\n",
    "\n",
    "- Search engine innovators like Yahoo! and Google needed to find a way to make sense of the massive amounts of data that their engines were collecting.\n",
    "- Hadoop was developed because it represented the most pragmatic way to allow companies to manage huge volumes of data easily.\n",
    "- Hadoop allowed big problems to be broken down into smaller elements so that analysis could be done quickly and cost-effectively.\n",
    "- By breaking the big data problem into small pieces that could be processed in parallel, you can process the information and regroup the small pieces to present results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Who use Hadoop?\n",
    "\n",
    "- Facebook uses Hadoop, Hive, and HB ase for data warehousing and real-time application serving.\n",
    "- Twitter uses Hadoop, Pig, and HB ase for data analysis, visualization, social graph analysis, and machine learning.\n",
    "- Yahoo! uses Hadoop for data analytics, machine learning, search ranking, email antispam, ad optimization...\n",
    "- eBay, Samsung, Rackspace, J.P. Morgan, Groupon, LinkedIn, AOL , Last.fm..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now let us try these commands:\n",
    "\n",
    "- `hadoop`\n",
    "- `echo $HADOOP_HOME`\n",
    "- `echo $HADOOP_CLASSPATH`\n",
    "- `echo $HADOOP_CONF_DIR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
      " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
      "  where CLASSNAME is a user-provided Java class\n",
      "\n",
      "  OPTIONS is none or any of:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "buildpaths                       attempt to add class files from build tree\n",
      "--config dir                     Hadoop config directory\n",
      "--debug                          turn on shell script debug mode\n",
      "--help                           usage information\n",
      "hostnames list[,of,host,names]   hosts to use in slave mode\n",
      "hosts filename                   list of hosts to use in slave mode\n",
      "loglevel level                   set the log4j level for this command\n",
      "workers                          turn on worker mode\n",
      "\n",
      "  SUBCOMMAND is one of:\n",
      "\n",
      "\n",
      "    Admin Commands:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "daemonlog     get/set the log level for each daemon\n",
      "\n",
      "    Client Commands:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "archive       create a Hadoop archive\n",
      "checknative   check native Hadoop and compression libraries availability\n",
      "classpath     prints the class path needed to get the Hadoop jar and the\n",
      "              required libraries\n",
      "conftest      validate configuration XML files\n",
      "credential    interact with credential providers\n",
      "distch        distributed metadata changer\n",
      "distcp        copy file or directories recursively\n",
      "dtutil        operations related to delegation tokens\n",
      "envvars       display computed Hadoop environment variables\n",
      "fs            run a generic filesystem user client\n",
      "gridmix       submit a mix of synthetic job, modeling a profiled from\n",
      "              production load\n",
      "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN\n",
      "              applications, not this command.\n",
      "jnipath       prints the java.library.path\n",
      "kdiag         Diagnose Kerberos Problems\n",
      "kerbname      show auth_to_local principal conversion\n",
      "key           manage keys via the KeyProvider\n",
      "rumenfolder   scale a rumen input trace\n",
      "rumentrace    convert logs into a rumen trace\n",
      "s3guard       manage metadata on S3\n",
      "trace         view and modify Hadoop tracing settings\n",
      "version       print the version\n",
      "\n",
      "    Daemon Commands:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "kms           run KMS, the Key Management Server\n",
      "\n",
      "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop 3.2.1\n",
      "Source code repository http://gitlab.alibaba-inc.com/soe/emr-hadoop.git -r fdbf79bb25ebd52e198bcc564baf822d0a6b7024\n",
      "Compiled by jenkins on 2021-07-15T07:57Z\n",
      "Compiled with protoc 2.5.0\n",
      "From source with checksum a727b26fa21579ad1b194bc17821d8\n",
      "This command was run using /opt/apps/ecm/service/hadoop/3.2.1-1.2.1/package/hadoop-3.2.1-1.2.1/share/hadoop/common/hadoop-common-3.2.1.jar\n"
     ]
    }
   ],
   "source": [
    "hadoop version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/hadoop-current\n"
     ]
    }
   ],
   "source": [
    "echo $HADOOP_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/java-1.8.0\n"
     ]
    }
   ],
   "source": [
    "echo $JAVA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/apps/extra-jars/*:/usr/lib/spark-current/yarn/spark-3.1.2-yarn-shuffle.jar:/opt/apps/extra-jars/*:/usr/lib/spark-current/yarn/spark-3.1.2-yarn-shuffle.jar\n"
     ]
    }
   ],
   "source": [
    "echo $HADOOP_CLASSPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/etc/ecm/hadoop-conf\n"
     ]
    }
   ],
   "source": [
    "echo $HADOOP_CONF_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two primary components of Hadoop\n",
    "\n",
    "- Hadoop Distributed File System (HDFS): A reliable, high-bandwidth, low-cost, data storage cluster that facilitates the management of related files across machines.\n",
    "- MapReduce engine: A high-performance parallel/distributed data-processing implementation of the MapReduce algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HDFS\n",
    "\n",
    "- HDFS is the world’s most reliable storage system.\n",
    "- It is a data service that offers a unique set of capabilities needed when data volumes and velocity are high. \n",
    "- The service includes a “NameNode” and multiple “data nodes”.\n",
    "\n",
    "### NameNodes\n",
    "\n",
    "- HDFS works by breaking large files into smaller pieces called blocks. The blocks are stored on data nodes, and it is the responsibility of the NameNode to know what blocks on which data nodes make up the complete file. \n",
    "- Data nodes are not very smart, but the NameNode is. The data nodes constantly ask the NameNode whether there is anything for them to do. This continuous behavior also tells the NameNode what data nodes are out there and how busy they are.\n",
    "\n",
    "### DataNodes\n",
    "\n",
    "- Within the HDFS cluster, data blocks are replicated across multiple data nodes and access is managed by the NameNode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Under the covers of HDFS\n",
    "\n",
    "- Big data brings the big challenges of volume, velocity, and variety. \n",
    "- HDFS addresses these challenges by breaking files into a related collection of smaller blocks. These blocks are distributed among the data nodes in the HDFS cluster and are managed by the NameNode. \n",
    "- Block sizes are configurable and are usually 128 megabytes (MB) or 256MB, meaning that a 1GB file consumes eight 128MB blocks for its basic storage needs. \n",
    "- HDFS is resilient, so these blocks are replicated throughout the cluster in case of a server failure. How does HDFS keep track of all these pieces? The short answer is file system metadata.\n",
    "- HDFS metadata is stored in the NameNode, and while the cluster is operating, all the metadata is loaded into the physical memory of the NameNode server.\n",
    "- Data nodes are very simplistic. They are servers that contain the blocks for a given set of files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data storage of HDFS\n",
    "![](./figs/hdfs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data storage of HDFS\n",
    "\n",
    "- Multiple copies of each block are stored across the cluster on different nodes. \n",
    "- This is a replication of data. By default, HDFS replication factor is 3. \n",
    "- It provides fault tolerance, reliability, and high availability.\n",
    "- A Large file is split into n number of small blocks. These blocks are stored at different nodes in the cluster in a distributed manner. Each block is replicated and stored across different nodes in the cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HDFS commands\n",
    "\n",
    "- `hadoop fs`\n",
    "- `hadoop fs -help`\n",
    "- `hadoop fs -ls /`\n",
    "- `hadoop fs -ls /user/student`\n",
    "- `hadoop fs -mv LICENSE license.txt`\n",
    "- `hadoop fs -mkdir yourNAME`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] [-v] [-x] <path> ...]\n",
      "\t[-expunge [-immediate]]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]\n",
      "\t[-head <file>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]\n",
      "\t[-ls2 [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\n",
      "\t[-rm2 [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] [-s <sleep interval>] <file>]\n",
      "\t[-test -[defswrz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touch [-a] [-m] [-t TIMESTAMP ] [-c] <path> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are:\n",
      "-conf <configuration file>        specify an application configuration file\n",
      "-D <property=value>               define a value for a given property\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
      "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
      "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
      "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
      "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
      "\n",
      "The general command line syntax is:\n",
      "command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "255",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "hadoop fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] [-v] [-x] <path> ...]\n",
      "\t[-expunge [-immediate]]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]\n",
      "\t[-head <file>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]\n",
      "\t[-ls2 [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\n",
      "\t[-rm2 [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] [-s <sleep interval>] <file>]\n",
      "\t[-test -[defswrz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touch [-a] [-m] [-t TIMESTAMP ] [-c] <path> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "-appendToFile <localsrc> ... <dst> :\n",
      "  Appends the contents of all the given local files to the given dst file. The dst\n",
      "  file will be created if it does not exist. If <localSrc> is -, then the input is\n",
      "  read from stdin.\n",
      "\n",
      "-cat [-ignoreCrc] <src> ... :\n",
      "  Fetch all files that match the file pattern <src> and display their content on\n",
      "  stdout.\n",
      "\n",
      "-checksum <src> ... :\n",
      "  Dump checksum information for files that match the file pattern <src> to stdout.\n",
      "  Note that this requires a round-trip to a datanode storing each block of the\n",
      "  file, and thus is not efficient to run on a large number of files. The checksum\n",
      "  of a file depends on its content, block size and the checksum algorithm and\n",
      "  parameters used for creating the file.\n",
      "  \n",
      "  For JindoFS only, block size can be configured for MD5 CRC algorithm\n",
      "    -b blockSize  Block size for computing MD5 CRC.\n",
      "\n",
      "-chgrp [-R] GROUP PATH... :\n",
      "  This is equivalent to -chown ... :GROUP ...\n",
      "\n",
      "-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH... :\n",
      "  Changes permissions of a file. This works similar to the shell's chmod command\n",
      "  with a few exceptions.\n",
      "                                                                                 \n",
      "  -R           modifies the files recursively. This is the only option currently \n",
      "               supported.                                                        \n",
      "  <MODE>       Mode is the same as mode used for the shell's command. The only   \n",
      "               letters recognized are 'rwxXt', e.g. +t,a+r,g-w,+rwx,o=r.         \n",
      "  <OCTALMODE>  Mode specifed in 3 or 4 digits. If 4 digits, the first may be 1 or\n",
      "               0 to turn the sticky bit on or off, respectively.  Unlike the     \n",
      "               shell command, it is not possible to specify only part of the     \n",
      "               mode, e.g. 754 is same as u=rwx,g=rx,o=r.                         \n",
      "  \n",
      "  If none of 'augo' is specified, 'a' is assumed and unlike the shell command, no\n",
      "  umask is applied.\n",
      "\n",
      "-chown [-R] [OWNER][:[GROUP]] PATH... :\n",
      "  Changes owner and group of a file. This is similar to the shell's chown command\n",
      "  with a few exceptions.\n",
      "                                                                                 \n",
      "  -R  modifies the files recursively. This is the only option currently          \n",
      "      supported.                                                                 \n",
      "  \n",
      "  If only the owner or group is specified, then only the owner or group is\n",
      "  modified. The owner and group names may only consist of digits, alphabet, and\n",
      "  any of [-_./@a-zA-Z0-9]. The names are case sensitive.\n",
      "  \n",
      "  WARNING: Avoid using '.' to separate user name and group though Linux allows it.\n",
      "  If user names have dots in them and you are using local file system, you might\n",
      "  see surprising results since the shell command 'chown' is used for local files.\n",
      "\n",
      "-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] <localsrc> ... <dst> :\n",
      "  Copy files from the local file system into fs. Copying fails if the file already\n",
      "  exists, unless the -f flag is given.\n",
      "  Flags:\n",
      "                                                                                 \n",
      "  -p                 Preserves access and modification times, ownership and the  \n",
      "                     mode.                                                       \n",
      "  -f                 Overwrites the destination if it already exists.            \n",
      "  -t <thread count>  Number of threads to be used, default is 1.                 \n",
      "  -l                 Allow DataNode to lazily persist the file to disk. Forces   \n",
      "                     replication factor of 1. This flag will result in reduced   \n",
      "                     durability. Use with care.                                  \n",
      "  -d                 Skip creation of temporary file(<dst>._COPYING_).           \n",
      "\n",
      "-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst> :\n",
      "  Identical to the -get command.\n",
      "\n",
      "-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] <path> ... :\n",
      "  Count the number of directories, files and bytes under the paths\n",
      "  that match the specified file pattern.  The output columns are:\n",
      "  DIR_COUNT FILE_COUNT CONTENT_SIZE PATHNAME\n",
      "  or, with the -q option:\n",
      "  QUOTA REM_QUOTA SPACE_QUOTA REM_SPACE_QUOTA\n",
      "        DIR_COUNT FILE_COUNT CONTENT_SIZE PATHNAME\n",
      "  The -h option shows file sizes in human readable format.\n",
      "  The -v option displays a header line.\n",
      "  The -x option excludes snapshots from being calculated. \n",
      "  The -t option displays quota by storage types.\n",
      "  It should be used with -q or -u option, otherwise it will be ignored.\n",
      "  If a comma-separated list of storage types is given after the -t option, \n",
      "  it displays the quota and usage for the specified types. \n",
      "  Otherwise, it displays the quota and usage for all the storage \n",
      "  types that support quota. The list of possible storage types(case insensitive):\n",
      "  ram_disk, ssd, disk and archive.\n",
      "  It can also pass the value '', 'all' or 'ALL' to specify all the storage types.\n",
      "  The -u option shows the quota and \n",
      "  the usage against the quota without the detailed content summary.The -e option\n",
      "  shows the erasure coding policy.\n",
      "\n",
      "-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst> :\n",
      "  Copy files that match the file pattern <src> to a destination.  When copying\n",
      "  multiple files, the destination must be a directory. Passing -p preserves status\n",
      "  [topax] (timestamps, ownership, permission, ACLs, XAttr). If -p is specified\n",
      "  with no <arg>, then preserves timestamps, ownership, permission. If -pa is\n",
      "  specified, then preserves permission also because ACL is a super-set of\n",
      "  permission. Passing -f overwrites the destination if it already exists. raw\n",
      "  namespace extended attributes are preserved if (1) they are supported (HDFS\n",
      "  only) and, (2) all of the source and target pathnames are in the /.reserved/raw\n",
      "  hierarchy. raw namespace xattr preservation is determined solely by the presence\n",
      "  (or absence) of the /.reserved/raw prefix and not by the -p option. Passing -d\n",
      "  will skip creation of temporary file(<dst>._COPYING_).\n",
      "\n",
      "-createSnapshot <snapshotDir> [<snapshotName>] :\n",
      "  Create a snapshot on a directory\n",
      "\n",
      "-deleteSnapshot <snapshotDir> <snapshotName> :\n",
      "  Delete a snapshot from a directory\n",
      "\n",
      "-df [-h] [<path> ...] :\n",
      "  Shows the capacity, free and used space of the filesystem. If the filesystem has\n",
      "  multiple partitions, and no path to a particular partition is specified, then\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  the status of the root partitions will be shown.\n",
      "                                                                                 \n",
      "  -h  Formats the sizes of files in a human-readable fashion rather than a number\n",
      "      of bytes.                                                                  \n",
      "\n",
      "-du [-s] [-h] [-v] [-x] <path> ... :\n",
      "  Show the amount of space, in bytes, used by the files that match the specified\n",
      "  file pattern. The following flags are optional:\n",
      "                                                                                 \n",
      "  -s  Rather than showing the size of each individual file that matches the      \n",
      "      pattern, shows the total (summary) size.                                   \n",
      "  -h  Formats the sizes of files in a human-readable fashion rather than a number\n",
      "      of bytes.                                                                  \n",
      "  -v  option displays a header line.                                             \n",
      "  -x  Excludes snapshots from being counted.                                     \n",
      "  \n",
      "  Note that, even without the -s option, this only shows size summaries one level\n",
      "  deep into a directory.\n",
      "  \n",
      "  The output is in the form \n",
      "  \tsize\tdisk space consumed\tname(full path)\n",
      "\n",
      "-expunge [-immediate] :\n",
      "  Delete files from the trash that are older than the retention threshold\n",
      "\n",
      "-find <path> ... <expression> ... :\n",
      "  Finds all files that match the specified expression and\n",
      "  applies selected actions to them. If no <path> is specified\n",
      "  then defaults to the current working directory. If no\n",
      "  expression is specified then defaults to -print.\n",
      "  \n",
      "  The following primary expressions are recognised:\n",
      "    -name pattern\n",
      "    -iname pattern\n",
      "      Evaluates as true if the basename of the file matches the\n",
      "      pattern using standard file system globbing.\n",
      "      If -iname is used then the match is case insensitive.\n",
      "  \n",
      "    -print\n",
      "    -print0\n",
      "      Always evaluates to true. Causes the current pathname to be\n",
      "      written to standard output followed by a newline. If the -print0\n",
      "      expression is used then an ASCII NULL character is appended rather\n",
      "      than a newline.\n",
      "  \n",
      "  The following operators are recognised:\n",
      "    expression -a expression\n",
      "    expression -and expression\n",
      "    expression expression\n",
      "      Logical AND operator for joining two expressions. Returns\n",
      "      true if both child expressions return true. Implied by the\n",
      "      juxtaposition of two expressions and so does not need to be\n",
      "      explicitly specified. The second expression will not be\n",
      "      applied if the first fails.\n",
      "\n",
      "-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst> :\n",
      "  Copy files that match the file pattern <src> to the local name.  <src> is kept. \n",
      "  When copying multiple files, the destination must be a directory. Passing -f\n",
      "  overwrites the destination if it already exists and -p preserves access and\n",
      "  modification times, ownership and the mode.\n",
      "\n",
      "-getfacl [-R] <path> :\n",
      "  Displays the Access Control Lists (ACLs) of files and directories. If a\n",
      "  directory has a default ACL, then getfacl also displays the default ACL.\n",
      "                                                                  \n",
      "  -R      List the ACLs of all files and directories recursively. \n",
      "  <path>  File or directory to list.                              \n",
      "\n",
      "-getfattr [-R] {-n name | -d} [-e en] <path> :\n",
      "  Displays the extended attribute names and values (if any) for a file or\n",
      "  directory.\n",
      "                                                                                 \n",
      "  -R             Recursively list the attributes for all files and directories.  \n",
      "  -n name        Dump the named extended attribute value.                        \n",
      "  -d             Dump all extended attribute values associated with pathname.    \n",
      "  -e <encoding>  Encode values after retrieving them.Valid encodings are \"text\", \n",
      "                 \"hex\", and \"base64\". Values encoded as text strings are enclosed\n",
      "                 in double quotes (\"), and values encoded as hexadecimal and     \n",
      "                 base64 are prefixed with 0x and 0s, respectively.               \n",
      "  <path>         The file or directory.                                          \n",
      "\n",
      "-getmerge [-nl] [-skip-empty-file] <src> <localdst> :\n",
      "  Get all the files in the directories that match the source file pattern and\n",
      "  merge and sort them to only one file on local fs. <src> is kept.\n",
      "                                                                     \n",
      "  -nl               Add a newline character at the end of each file. \n",
      "  -skip-empty-file  Do not add new line character for empty file.    \n",
      "\n",
      "-head <file> :\n",
      "  Show the first 1KB of the file.\n",
      "\n",
      "-help [cmd ...] :\n",
      "  Displays help for given command or all commands if none is specified.\n",
      "\n",
      "-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...] :\n",
      "  List the contents that match the specified file pattern. If path is not\n",
      "  specified, the contents of /user/<currentUser> will be listed. For a directory a\n",
      "  list of its direct children is returned (unless -d option is specified).\n",
      "  \n",
      "  Directory entries are of the form:\n",
      "  \tpermissions - userId groupId sizeOfDirectory(in bytes)\n",
      "  modificationDate(yyyy-MM-dd HH:mm) directoryName\n",
      "  \n",
      "  and file entries are of the form:\n",
      "  \tpermissions numberOfReplicas userId groupId sizeOfFile(in bytes)\n",
      "  modificationDate(yyyy-MM-dd HH:mm) fileName\n",
      "  \n",
      "    -C  Display the paths of files and directories only.\n",
      "    -d  Directories are listed as plain files.\n",
      "    -h  Formats the sizes of files in a human-readable fashion\n",
      "        rather than a number of bytes.\n",
      "    -q  Print ? instead of non-printable characters.\n",
      "    -R  Recursively list the contents of directories.\n",
      "    -t  Sort files by modification time (most recent first).\n",
      "    -S  Sort files by size.\n",
      "    -r  Reverse the order of the sort.\n",
      "    -u  Use time of last access instead of modification for\n",
      "  display and sorting.  -e  Show extended info(StorageClass) of files, only\n",
      "  JindoFS (Including oss:// via JindoFS) support this.      To enable this feature\n",
      "  by default, you can use ls2 instead of ls,           or set\n",
      "  \"hadoop.shell.ls.show.extended.enabled\" with \"true\" to core-site.xml; \n",
      "\n",
      "-ls2 [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...] :\n",
      "  List the contents that match the specified file pattern. If path is not\n",
      "  specified, the contents of /user/<currentUser> will be listed. For a directory a\n",
      "  list of its direct children is returned (unless -d option is specified).\n",
      "  \n",
      "  Directory entries are of the form:\n",
      "  \tpermissions - userId groupId sizeOfDirectory(in bytes)\n",
      "  modificationDate(yyyy-MM-dd HH:mm) directoryName\n",
      "  \n",
      "  and file entries are of the form:\n",
      "  \tpermissions numberOfReplicas userId groupId sizeOfFile(in bytes)\n",
      "  modificationDate(yyyy-MM-dd HH:mm) fileName\n",
      "  \n",
      "    -C  Display the paths of files and directories only.\n",
      "    -d  Directories are listed as plain files.\n",
      "    -h  Formats the sizes of files in a human-readable fashion\n",
      "        rather than a number of bytes.\n",
      "    -q  Print ? instead of non-printable characters.\n",
      "    -R  Recursively list the contents of directories.\n",
      "    -t  Sort files by modification time (most recent first).\n",
      "    -S  Sort files by size.\n",
      "    -r  Reverse the order of the sort.\n",
      "    -u  Use time of last access instead of modification for\n",
      "  display and sorting.  -e  Show extended info(StorageClass) of files, only\n",
      "  JindoFS (Including oss:// via JindoFS) support this.      To enable this feature\n",
      "  by default, you can use ls2 instead of ls,           or set\n",
      "  \"hadoop.shell.ls.show.extended.enabled\" with \"true\" to core-site.xml; \n",
      "\n",
      "-mkdir [-p] <path> ... :\n",
      "  Create a directory in specified location.\n",
      "                                                  \n",
      "  -p  Do not fail if the directory already exists \n",
      "\n",
      "-moveFromLocal <localsrc> ... <dst> :\n",
      "  Same as -put, except that the source is deleted after it's copied.\n",
      "\n",
      "-moveToLocal <src> <localdst> :\n",
      "  Not implemented yet\n",
      "\n",
      "-mv <src> ... <dst> :\n",
      "  Move files that match the specified file pattern <src> to a destination <dst>. \n",
      "  When moving multiple files, the destination must be a directory.\n",
      "\n",
      "-put [-f] [-p] [-l] [-d] <localsrc> ... <dst> :\n",
      "  Copy files from the local file system into fs. Copying fails if the file already\n",
      "  exists, unless the -f flag is given.\n",
      "  Flags:\n",
      "                                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -p  Preserves access and modification times, ownership and the mode. \n",
      "  -f  Overwrites the destination if it already exists.                 \n",
      "  -l  Allow DataNode to lazily persist the file to disk. Forces        \n",
      "         replication factor of 1. This flag will result in reduced\n",
      "         durability. Use with care.\n",
      "                                                        \n",
      "  -d  Skip creation of temporary file(<dst>._COPYING_). \n",
      "\n",
      "-renameSnapshot <snapshotDir> <oldName> <newName> :\n",
      "  Rename a snapshot from oldName to newName\n",
      "\n",
      "-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ... :\n",
      "  Delete all files that match the specified file pattern. Equivalent to the Unix\n",
      "  command \"rm <src>\"\n",
      "                                                                                 \n",
      "  -f          If the file does not exist, do not display a diagnostic message or \n",
      "              modify the exit status to reflect an error.                        \n",
      "  -[rR]       Recursively deletes directories.                                   \n",
      "  -skipTrash  option bypasses trash, if enabled, and immediately deletes <src>.  \n",
      "  -safely     option requires safety confirmation, if enabled, requires          \n",
      "              confirmation before deleting large directory with more than        \n",
      "              <hadoop.shell.delete.limit.num.files> files. Delay is expected when\n",
      "              walking over large directory recursively to count the number of    \n",
      "              files to be deleted before the confirmation.                       \n",
      "\n",
      "-rm2 [-f] [-r|-R] [-skipTrash] [-safely] <src> ... :\n",
      "  Delete all files that match the specified file pattern. Equivalent to the Unix\n",
      "  command \"rm <src>\"\n",
      "                                                                                 \n",
      "  -f          If the file does not exist, do not display a diagnostic message or \n",
      "              modify the exit status to reflect an error.                        \n",
      "  -[rR]       Recursively deletes directories.                                   \n",
      "  -skipTrash  option bypasses trash, if enabled, and immediately deletes <src>.  \n",
      "  -safely     option requires safety confirmation, if enabled, requires          \n",
      "              confirmation before deleting large directory with more than        \n",
      "              <hadoop.shell.delete.limit.num.files> files. Delay is expected when\n",
      "              walking over large directory recursively to count the number of    \n",
      "              files to be deleted before the confirmation.                       \n",
      "\n",
      "-rmdir [--ignore-fail-on-non-empty] <dir> ... :\n",
      "  Removes the directory entry specified by each directory argument, provided it is\n",
      "  empty.\n",
      "\n",
      "-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>] :\n",
      "  Sets Access Control Lists (ACLs) of files and directories.\n",
      "  Options:\n",
      "                                                                                 \n",
      "  -b          Remove all but the base ACL entries. The entries for user, group   \n",
      "              and others are retained for compatibility with permission bits.    \n",
      "  -k          Remove the default ACL.                                            \n",
      "  -R          Apply operations to all files and directories recursively.         \n",
      "  -m          Modify ACL. New entries are added to the ACL, and existing entries \n",
      "              are retained.                                                      \n",
      "  -x          Remove specified ACL entries. Other ACL entries are retained.      \n",
      "  --set       Fully replace the ACL, discarding all existing entries. The        \n",
      "              <acl_spec> must include entries for user, group, and others for    \n",
      "              compatibility with permission bits. If the ACL spec contains only  \n",
      "              access entries, then the existing default entries are retained. If \n",
      "              the ACL spec contains only default entries, then the existing      \n",
      "              access entries are retained. If the ACL spec contains both access  \n",
      "              and default entries, then both are replaced.                       \n",
      "  <acl_spec>  Comma separated list of ACL entries.                               \n",
      "  <path>      File or directory to modify.                                       \n",
      "\n",
      "-setfattr {-n name [-v value] | -x name} <path> :\n",
      "  Sets an extended attribute name and value for a file or directory.\n",
      "                                                                                 \n",
      "  -n name   The extended attribute name.                                         \n",
      "  -v value  The extended attribute value. There are three different encoding     \n",
      "            methods for the value. If the argument is enclosed in double quotes, \n",
      "            then the value is the string inside the quotes. If the argument is   \n",
      "            prefixed with 0x or 0X, then it is taken as a hexadecimal number. If \n",
      "            the argument begins with 0s or 0S, then it is taken as a base64      \n",
      "            encoding.                                                            \n",
      "  -x name   Remove the extended attribute.                                       \n",
      "  <path>    The file or directory.                                               \n",
      "\n",
      "-setrep [-R] [-w] <rep> <path> ... :\n",
      "  Set the replication level of a file. If <path> is a directory then the command\n",
      "  recursively changes the replication factor of all files under the directory tree\n",
      "  rooted at <path>. The EC files will be ignored here.\n",
      "                                                                                 \n",
      "  -w  It requests that the command waits for the replication to complete. This   \n",
      "      can potentially take a very long time.                                     \n",
      "  -R  It is accepted for backwards compatibility. It has no effect.              \n",
      "\n",
      "-stat [format] <path> ... :\n",
      "  Print statistics about the file/directory at <path>\n",
      "  in the specified format. Format accepts permissions in\n",
      "  octal (%a) and symbolic (%A), filesize in\n",
      "  bytes (%b), type (%F), group name of owner (%g),\n",
      "  name (%n), block size (%o), replication (%r), user name\n",
      "  of owner (%u), access date (%x, %X).\n",
      "  modification date (%y, %Y).\n",
      "  %x and %y show UTC date as \"yyyy-MM-dd HH:mm:ss\" and\n",
      "  %X and %Y show milliseconds since January 1, 1970 UTC.\n",
      "  If the format is not specified, %y is used by default.\n",
      "\n",
      "-tail [-f] [-s <sleep interval>] <file> :\n",
      "  Show the last 1KB of the file.\n",
      "                                                                               \n",
      "  -f  Shows appended data as the file grows.                                   \n",
      "  -s  With -f , defines the sleep interval between iterations in milliseconds. \n",
      "\n",
      "-test -[defswrz] <path> :\n",
      "  Answer various questions about <path>, with result via exit status.\n",
      "    -d  return 0 if <path> is a directory.\n",
      "    -e  return 0 if <path> exists.\n",
      "    -f  return 0 if <path> is a file.\n",
      "    -s  return 0 if file <path> is greater         than zero bytes in size.\n",
      "    -w  return 0 if file <path> exists         and write permission is granted.\n",
      "    -r  return 0 if file <path> exists         and read permission is granted.\n",
      "    -z  return 0 if file <path> is         zero bytes in size, else return 1.\n",
      "\n",
      "-text [-ignoreCrc] <src> ... :\n",
      "  Takes a source file and outputs the file in text format.\n",
      "  The allowed formats are zip and TextRecordInputStream and Avro.\n",
      "\n",
      "-touch [-a] [-m] [-t TIMESTAMP ] [-c] <path> ... :\n",
      "  Updates the access and modification times of the file specified by the <path> to\n",
      "  the current time. If the file does not exist, then a zero length file is created\n",
      "  at <path> with current time as the timestamp of that <path>.\n",
      "  -a Change only the access time \n",
      "  -m Change only the modification time \n",
      "  -t TIMESTAMP Use specified timestamp (in format yyyyMMddHHmmss) instead of\n",
      "  current time \n",
      "  -c Do not create any files\n",
      "\n",
      "-touchz <path> ... :\n",
      "  Creates a file of zero length at <path> with current time as the timestamp of\n",
      "  that <path>. An error is returned if the file exists with non-zero length\n",
      "\n",
      "-truncate [-w] <length> <path> ... :\n",
      "  Truncate all files that match the specified file pattern to the specified\n",
      "  length.\n",
      "                                                                                 \n",
      "  -w  Requests that the command wait for block recovery to complete, if          \n",
      "      necessary.                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-usage [cmd ...] :\n",
      "  Displays the usage for given command or all commands if none is specified.\n",
      "\n",
      "Generic options supported are:\n",
      "-conf <configuration file>        specify an application configuration file\n",
      "-D <property=value>               define a value for a given property\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
      "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
      "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
      "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
      "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
      "\n",
      "The general command line syntax is:\n",
      "command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\n",
      "drwxr-xr-x   - hadoop    hadoop          0 2021-11-18 10:51 /apps\n",
      "drwxrwxrwx   - flowagent hadoop          0 2021-11-18 10:51 /emr-flow\n",
      "drwxr-x--x   - root      hadoop          0 2021-11-18 10:51 /emr-sparksql-udf\n",
      "drwxr-x--x   - hadoop    hadoop          0 2021-11-18 10:51 /spark-history\n",
      "drwxrwxrwt   - root      hadoop          0 2021-12-03 11:56 /tmp\n",
      "drwxr-x--t   - hadoop    hadoop          0 2021-12-03 11:56 /user\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwxr-x--x   - hadoop  hadoop          0 2021-11-18 10:53 /user/hadoop\n",
      "drwxr-x--x   - hadoop  hadoop          0 2021-11-18 10:51 /user/hive\n",
      "drwxr-x--x   - student hadoop          0 2021-12-03 11:56 /user/student\n",
      "drwxr-x--x   - student hadoop          0 2021-12-03 12:03 /user/yanfei\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -ls /user/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/12/03 11:57:27 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -put /home/yanfei/lectures/BDE-L7-hadoop.ipynb ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r-----   2 yanfei hadoop      68738 2021-12-03 11:57 /user/yanfei/BDE-L7-hadoop.ipynb\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -ls /user/yanfei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "hadoop fs -mv BDE-L7-hadoop.ipynb hadoop.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r-----   2 yanfei hadoop      68738 2021-12-03 11:57 /user/yanfei/hadoop.ipynb\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -ls /user/yanfei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hadoop MapReduce\n",
    "\n",
    "- Google released a paper on MapReduce technology in December, 2004. This became the genesis of the Hadoop Processing Model. \n",
    "- MapReduce is a batch-based, distributed computing framework.\n",
    "- It allows you to parallelize work over a large amount of raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Traditional way\n",
    "\n",
    "![](./figs/traditional.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Traditional way\n",
    "\n",
    "- Critical path problem: It is the amount of time taken to finish the job without delaying the next milestone or actual completion date. So, if, any of the machines delays the job, the whole work gets delayed.\n",
    "- Reliability problem: What if, any of the machines which is working with a part of data fails? The management of this failover becomes a challenge.\n",
    "- Equal split issue: How will I divide the data into smaller chunks so that each machine gets even part of data to work with. In other words, how to equally divide the data such that no individual machine is overloaded or under utilized. \n",
    "- Single split may fail: If any of the machine fails to provide the output, I will not be able to calculate the result. So, there should be a mechanism to ensure this fault tolerance capability of the system.\n",
    "- Aggregation of result: There should be a mechanism to aggregate the result generated by each of the machines to produce the final output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce\n",
    "\n",
    "MapReduce gives you the flexibility to write code logic without caring about the design issues of the system. \n",
    "\n",
    "![](./figs/mapreduce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce\n",
    "\n",
    "MapReduce is a programming framework that allows us to perform distributed and parallel processing on large data sets in a distributed environment.\n",
    "\n",
    "- MapReduce consists of two distinct tasks – Map and Reduce.\n",
    "- As the name MapReduce suggests, reducer phase takes place after mapper phase has been completed.\n",
    "- So, the first is the map job, where a block of data is read and processed to produce key-value pairs as intermediate outputs.\n",
    "- The output of a Mapper or map job (key-value pairs) is input to the Reducer.\n",
    "- The reducer receives the key-value pair from multiple map jobs.\n",
    "- Then, the reducer aggregates those intermediate data tuples (intermediate key-value pair) into a smaller set of tuples or key-value pairs which is the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce example - word count\n",
    "\n",
    "![](./figs/wordcount.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Divide the input in three splits as shown in the figure. This will distribute the work among all the map nodes.\n",
    "2. Tokenize the words in each of the mapper and give a hardcoded value (1) to each of the tokens or words. The rationale behind giving a hardcoded value equal to 1 is that every word, in itself, will occur once.\n",
    "3. Now, a list of key-value pair will be created where the key is nothing but the individual words and value is one. So, for the first line (Dear Bear River) we have 3 key-value pairs – Dear, 1; Bear, 1; River, 1. The mapping process remains the same on all the nodes.\n",
    "4. After mapper phase, a partition process takes place where sorting and shuffling happens so that all the tuples with the same key are sent to the corresponding reducer.\n",
    "5. After the sorting and shuffling phase, each reducer will have a unique key and a list of values corresponding to that very key. For example, Bear, [1,1]; Car, [1,1,1].., etc.\n",
    "6. Now, each Reducer counts the values which are present in that list of values. As shown in the figure, reducer gets a list of values which is [1,1] for the key Bear. Then, it counts the number of ones in the very list and gives the final output as – Bear, 2.\n",
    "7. Finally, all the output key/value pairs are then collected and written in the output file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce example - word count \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/yanfei/output': No such file or directory\n",
      "packageJobJar: [/tmp/hadoop-unjar4923265696776924492/] [] /tmp/streamjob1242618694333838615.jar tmpDir=null\n",
      "21/12/08 11:16:06 INFO client.RMProxy: Connecting to ResourceManager at emr-header-1.cluster-49012/192.168.0.3:8032\n",
      "21/12/08 11:16:06 INFO client.AHSProxy: Connecting to Application History server at emr-header-1.cluster-49012/192.168.0.3:10200\n",
      "21/12/08 11:16:06 INFO client.RMProxy: Connecting to ResourceManager at emr-header-1.cluster-49012/192.168.0.3:8032\n",
      "21/12/08 11:16:06 INFO client.AHSProxy: Connecting to Application History server at emr-header-1.cluster-49012/192.168.0.3:10200\n",
      "21/12/08 11:16:06 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/yanfei/.staging/job_1637546690590_0007\n",
      "21/12/08 11:16:06 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "21/12/08 11:16:06 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library from the embedded binaries\n",
      "21/12/08 11:16:06 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 97184efe294f64a51a4c5c172cbc22146103da53]\n",
      "21/12/08 11:16:06 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "21/12/08 11:16:06 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "21/12/08 11:16:06 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "21/12/08 11:16:06 INFO mapreduce.JobSubmitter: number of splits:16\n",
      "21/12/08 11:16:06 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "21/12/08 11:16:06 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1637546690590_0007\n",
      "21/12/08 11:16:06 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "21/12/08 11:16:06 INFO conf.Configuration: resource-types.xml not found\n",
      "21/12/08 11:16:06 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "21/12/08 11:16:06 INFO impl.YarnClientImpl: Submitted application application_1637546690590_0007\n",
      "21/12/08 11:16:07 INFO mapreduce.Job: The url to track the job: http://emr-header-1.cluster-49012:20888/proxy/application_1637546690590_0007/\n",
      "21/12/08 11:16:07 INFO mapreduce.Job: Running job: job_1637546690590_0007\n",
      "21/12/08 11:16:12 INFO mapreduce.Job: Job job_1637546690590_0007 running in uber mode : false\n",
      "21/12/08 11:16:12 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "21/12/08 11:16:17 INFO mapreduce.Job:  map 88% reduce 0%\n",
      "21/12/08 11:16:18 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "21/12/08 11:16:21 INFO mapreduce.Job:  map 100% reduce 43%\n",
      "21/12/08 11:16:22 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "21/12/08 11:16:23 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "21/12/08 11:16:23 INFO mapreduce.Job: Job job_1637546690590_0007 completed successfully\n",
      "21/12/08 11:16:23 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=24233\n",
      "\t\tFILE: Number of bytes written=5740501\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=132018\n",
      "\t\tHDFS: Number of bytes written=175\n",
      "\t\tHDFS: Number of read operations=83\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=21\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=16\n",
      "\t\tLaunched reduce tasks=7\n",
      "\t\tData-local map tasks=16\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4465986\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1397923\n",
      "\t\tTotal time spent by all map tasks (ms)=39522\n",
      "\t\tTotal time spent by all reduce tasks (ms)=12371\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=39522\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=12371\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=142911552\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=44733536\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1535\n",
      "\t\tMap output records=1535\n",
      "\t\tMap output bytes=70324\n",
      "\t\tMap output materialized bytes=37621\n",
      "\t\tInput split bytes=1840\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=799\n",
      "\t\tReduce shuffle bytes=37621\n",
      "\t\tReduce input records=1535\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=3070\n",
      "\t\tShuffled Maps =112\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=112\n",
      "\t\tGC time elapsed (ms)=1292\n",
      "\t\tCPU time spent (ms)=14690\n",
      "\t\tPhysical memory (bytes) snapshot=10082455552\n",
      "\t\tVirtual memory (bytes) snapshot=116097613824\n",
      "\t\tTotal committed heap usage (bytes)=17024155648\n",
      "\t\tPeak Map Physical memory (bytes)=499752960\n",
      "\t\tPeak Map Virtual memory (bytes)=5044006912\n",
      "\t\tPeak Reduce Physical memory (bytes)=307265536\n",
      "\t\tPeak Reduce Virtual memory (bytes)=5060501504\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=130178\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=175\n",
      "21/12/08 11:16:23 INFO streaming.StreamJob: Output directory: /user/yanfei/output\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -rm -r /user/yanfei/output\n",
    "\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
    "    -input /user/yanfei/hadoop.ipynb \\\n",
    "    -output /user/yanfei/output \\\n",
    "    -mapper \"/usr/bin/cat\" \\\n",
    "    -reducer \"/usr/bin/wc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 items\n",
      "-rw-r-----   2 yanfei hadoop          0 2021-12-03 12:02 /user/yanfei/output/_SUCCESS\n",
      "-rw-r-----   2 yanfei hadoop         25 2021-12-03 12:02 /user/yanfei/output/part-00000\n",
      "-rw-r-----   2 yanfei hadoop         25 2021-12-03 12:02 /user/yanfei/output/part-00001\n",
      "-rw-r-----   2 yanfei hadoop         25 2021-12-03 12:02 /user/yanfei/output/part-00002\n",
      "-rw-r-----   2 yanfei hadoop         25 2021-12-03 12:02 /user/yanfei/output/part-00003\n",
      "-rw-r-----   2 yanfei hadoop         25 2021-12-03 12:02 /user/yanfei/output/part-00004\n",
      "-rw-r-----   2 yanfei hadoop         25 2021-12-03 12:02 /user/yanfei/output/part-00005\n",
      "-rw-r-----   2 yanfei hadoop         25 2021-12-03 12:02 /user/yanfei/output/part-00006\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hadoop fs -ls /user/yanfei/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/12/03 12:02:41 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "    256    1115    9707\t\n",
      "21/12/03 12:02:41 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "    275     933    9072\t\n",
      "    232    1280   11692\t\n",
      "    167    1052    9170\t\n",
      "    232    1180   11013\t\n",
      "    190    1214   10729\t\n",
      "    183    1000    8890\t\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -cat /user/yanfei/output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce example - word count \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/12/03 12:02:57 INFO fs.TrashPolicyDefault: Moved: 'hdfs://emr-header-1.cluster-49012:9000/user/yanfei/output' to trash at: hdfs://emr-header-1.cluster-49012:9000/user/yanfei/.Trash/Current/user/yanfei/output\n",
      "packageJobJar: [/tmp/hadoop-unjar4915498231992068313/] [] /tmp/streamjob2060219075808971674.jar tmpDir=null\n",
      "21/12/03 12:02:59 INFO client.RMProxy: Connecting to ResourceManager at emr-header-1.cluster-49012/192.168.0.3:8032\n",
      "21/12/03 12:02:59 INFO client.AHSProxy: Connecting to Application History server at emr-header-1.cluster-49012/192.168.0.3:10200\n",
      "21/12/03 12:02:59 INFO client.RMProxy: Connecting to ResourceManager at emr-header-1.cluster-49012/192.168.0.3:8032\n",
      "21/12/03 12:02:59 INFO client.AHSProxy: Connecting to Application History server at emr-header-1.cluster-49012/192.168.0.3:10200\n",
      "21/12/03 12:02:59 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/yanfei/.staging/job_1637546690590_0004\n",
      "21/12/03 12:02:59 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "21/12/03 12:03:00 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library from the embedded binaries\n",
      "21/12/03 12:03:00 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 97184efe294f64a51a4c5c172cbc22146103da53]\n",
      "21/12/03 12:03:00 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "21/12/03 12:03:00 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "21/12/03 12:03:00 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "21/12/03 12:03:00 INFO mapreduce.JobSubmitter: number of splits:16\n",
      "21/12/03 12:03:00 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "21/12/03 12:03:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1637546690590_0004\n",
      "21/12/03 12:03:00 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "21/12/03 12:03:00 INFO conf.Configuration: resource-types.xml not found\n",
      "21/12/03 12:03:00 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "21/12/03 12:03:00 INFO impl.YarnClientImpl: Submitted application application_1637546690590_0004\n",
      "21/12/03 12:03:00 INFO mapreduce.Job: The url to track the job: http://emr-header-1.cluster-49012:20888/proxy/application_1637546690590_0004/\n",
      "21/12/03 12:03:00 INFO mapreduce.Job: Running job: job_1637546690590_0004\n",
      "21/12/03 12:03:05 INFO mapreduce.Job: Job job_1637546690590_0004 running in uber mode : false\n",
      "21/12/03 12:03:05 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "21/12/03 12:03:09 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "21/12/03 12:03:10 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "21/12/03 12:03:12 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "21/12/03 12:03:12 INFO mapreduce.Job: Job job_1637546690590_0004 completed successfully\n",
      "21/12/03 12:03:12 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=16539\n",
      "\t\tFILE: Number of bytes written=4237750\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=132018\n",
      "\t\tHDFS: Number of bytes written=25\n",
      "\t\tHDFS: Number of read operations=53\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=16\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=16\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4745661\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=189727\n",
      "\t\tTotal time spent by all map tasks (ms)=41997\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1679\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=41997\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1679\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=151861152\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=6071264\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1535\n",
      "\t\tMap output records=1535\n",
      "\t\tMap output bytes=70324\n",
      "\t\tMap output materialized bytes=25405\n",
      "\t\tInput split bytes=1840\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=799\n",
      "\t\tReduce shuffle bytes=25405\n",
      "\t\tReduce input records=1535\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=3070\n",
      "\t\tShuffled Maps =16\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=16\n",
      "\t\tGC time elapsed (ms)=1050\n",
      "\t\tCPU time spent (ms)=11930\n",
      "\t\tPhysical memory (bytes) snapshot=8222433280\n",
      "\t\tVirtual memory (bytes) snapshot=85753057280\n",
      "\t\tTotal committed heap usage (bytes)=12587630592\n",
      "\t\tPeak Map Physical memory (bytes)=495804416\n",
      "\t\tPeak Map Virtual memory (bytes)=5044715520\n",
      "\t\tPeak Reduce Physical memory (bytes)=303345664\n",
      "\t\tPeak Reduce Virtual memory (bytes)=5058519040\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=130178\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=25\n",
      "21/12/03 12:03:12 INFO streaming.StreamJob: Output directory: /user/yanfei/output\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -rm -r /user/yanfei/output\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
    "    -input /user/yanfei/hadoop.ipynb \\\n",
    "    -output /user/yanfei/output \\\n",
    "    -mapper \"/usr/bin/cat\" \\\n",
    "    -reducer \"/usr/bin/wc\" \\\n",
    "    -numReduceTasks 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/12/03 12:03:20 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "   1535    7774   70273\t\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -cat /user/yanfei/output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Further readings\n",
    "\n",
    "[MapReduce tutorial.](https://hadoop.apache.org/docs/r3.2.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "rise": {
   "autolaunch": true,
   "footer": "<h3>Big Data Essential, 2020 Autumn</h3>",
   "scroll": true,
   "theme": "solarized",
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
